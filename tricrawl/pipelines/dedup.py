"""
ì¤‘ë³µ ê²Œì‹œë¬¼ í•„í„°ë§ íŒŒì´í”„ë¼ì¸
ì´ë¯¸ ì•Œë¦¼ì„ ë³´ë‚¸ ê²Œì‹œë¬¼ì€ ë‹¤ì‹œ ë³´ë‚´ì§€ ì•ŠìŒ
"""
import json
import hashlib
import time
import requests
from collections import OrderedDict
from pathlib import Path
from scrapy.exceptions import DropItem
import structlog

logger = structlog.get_logger(__name__)


class DeduplicationPipeline:
    """
    ì œëª© í•´ì‹œ ê¸°ë°˜ ì¤‘ë³µ í•„í„°ë§
    - JSON ìºì‹œë¥¼ ì‚¬ìš©í•˜ë˜, ê°œìˆ˜/ê¸°ê°„ ìƒí•œìœ¼ë¡œ ë¬´í•œ ëˆ„ì  ë°©ì§€
    - ì˜¤ë˜ëœ í•­ëª©ì€ ìˆœì„œ ê¸°ì¤€ìœ¼ë¡œ ì œê±°(OrderedDict)
    """

    def __init__(self, data_dir: Path, max_entries: int, max_days: int):
        self.data_dir = data_dir
        self.data_dir.mkdir(parents=True, exist_ok=True)

        # ìƒí•œ ì„¤ì •(0 ì´í•˜ë©´ ë¹„í™œì„±)
        self.max_entries = max_entries
        self.max_days = max_days

        # ì‚½ì… ìˆœì„œ ìœ ì§€(ì™¼ìª½ì´ ê°€ì¥ ì˜¤ë˜ëœ í•­ëª©)
        self.seen_hashes = OrderedDict()
        self.cache_file = None

        # ìŠ¤íŒŒì´ë”/í¬ë¡¤ëŸ¬ ì°¸ì¡°(Scrapy deprecation ëŒ€ì‘)
        self._crawler = None
        self._spider = None

        # ì•Œë¦¼ ì„¤ì •(Webhook ë¯¸ì„¤ì •ì´ë©´ ì•Œë¦¼ ìŠ¤í‚µ)
        self._webhook_url = None
        self._notify_on_no_new = True

        # ì‹¤í–‰ ìš”ì•½ ì¹´ìš´í„°
        self.total_items = 0
        self.new_items = 0
        self.duplicate_items = 0

        # ì‹¤í–‰ ì¤‘ í™•ì¸ëœ í•´ì‹œ(ì¤‘ë³µ í¬í•¨)
        self._seen_this_run = set()

        # ì‹¤í–‰ ì¤‘ ë³´ì´ì§€ ì•Šì€ ìºì‹œ ì œê±° ì˜µì…˜
        self._prune_unseen = False

    @classmethod
    def from_crawler(cls, crawler):
        # í”„ë¡œì íŠ¸ ë£¨íŠ¸ì˜ data/ í´ë”
        data_dir = Path("data")
        # MVP ë‹¨ê³„ ê¸°ë³¸ê°’(í•„ìš” ì‹œ settings.pyì—ì„œ ì¡°ì •)
        max_entries = crawler.settings.getint("DEDUP_MAX_ENTRIES", 20000)
        max_days = crawler.settings.getint("DEDUP_MAX_DAYS", 30)

        pipeline = cls(data_dir, max_entries, max_days)
        pipeline._crawler = crawler
        pipeline._webhook_url = crawler.settings.get("DISCORD_WEBHOOK_URL")
        pipeline._notify_on_no_new = crawler.settings.getbool("NOTIFY_ON_NO_NEW_DATA", True)
        pipeline._prune_unseen = crawler.settings.getbool("DEDUP_PRUNE_UNSEEN", False)
        return pipeline

    def open_spider(self, spider=None):
        # ìŠ¤íŒŒì´ë” ì‹œì‘ ì‹œ ì „ìš© ì¤‘ë³µ ìºì‹œ ë¡œë“œ
        spider_obj = self._resolve_spider(spider)
        spider_name = spider_obj.name if spider_obj else "unknown"

        # ì‹¤í–‰ë³„ ì¹´ìš´í„° ì´ˆê¸°í™”
        self.total_items = 0
        self.new_items = 0
        self.duplicate_items = 0
        self._seen_this_run = set()

        self.cache_file = self.data_dir / f"dedup_{spider_name}.json"
        self.load_cache(spider_name)
        self._prune_cache()

    def get_hash(self, item):
        """
        ê²Œì‹œë¬¼ ê³ ìœ  í•´ì‹œ ìƒì„±
        1. item['dedup_id'] ì¡´ì¬ ì‹œ ìµœìš°ì„  ì‚¬ìš©(ìŠ¤íŒŒì´ë” ì •ì˜ ìœ ë‹ˆí¬ í‚¤)
        2. ì—†ì„ ê²½ìš° ì œëª© + ì‘ì„±ì
        3. ìŠ¤íŒŒì´ë”ì—ì„œ ì§€ì •í•œ IDê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
        """
        custom_id = item.get("dedup_id")
        if custom_id:
            return custom_id

        title = item.get("title", "")
        author = item.get("author", "")
        key = f"{title}|{author}"
        gen_hash = hashlib.md5(key.encode()).hexdigest()
        
        # ìƒì„±ëœ í•´ì‹œë¥¼ ì•„ì´í…œì— ê¸°ë¡(í›„ì† íŒŒì´í”„ë¼ì¸ ì°¸ì¡°ìš©)
        item["dedup_id"] = gen_hash
        return gen_hash

    def process_item(self, item, spider=None):
        """ì¤‘ë³µ ê²€ì‚¬ í›„ í†µê³¼ ë˜ëŠ” ë“œë¡­"""
        item_hash = self.get_hash(item)
        self.total_items += 1
        self._seen_this_run.add(item_hash)

        if self._hash_exists(item_hash):
            # ì´ë¯¸ ë³¸ ê²Œì‹œë¬¼ -> ìŠ¤í‚µ
            self.duplicate_items += 1
            raise DropItem(f"Duplicate: {item.get('title', '')[:30]}")

        # ìƒˆ ê²Œì‹œë¬¼ -> ìºì‹œì— ì¶”ê°€
        self.new_items += 1
        self._add_hash(item_hash)
        return item

    def close_spider(self, spider=None):
        # ìŠ¤íŒŒì´ë” ì¢…ë£Œ ì‹œ ìºì‹œ ì €ì¥ + ìš”ì•½ ë¡œê·¸/ì•Œë¦¼
        spider_obj = self._resolve_spider(spider)
        spider_name = spider_obj.name if spider_obj else "unknown"

        if self._prune_unseen and self._seen_this_run:
            self._prune_unseen_entries()

        self.save_cache(spider_name)
        self._log_summary(spider_name)
        self._notify_no_new(spider_name)

    def load_cache(self, spider_name):
        try:
            if self.cache_file and self.cache_file.exists():
                with open(self.cache_file, "r", encoding="utf-8") as f:
                    data = json.load(f) or {}

                now = time.time()

                # í•´ì‹œ í¬ë§·: entries = [{"hash": "...", "ts": 1700000000}, ...]
                entries = data.get("entries")
                if isinstance(entries, list):
                    for entry in entries:
                        if not isinstance(entry, dict):
                            continue
                        entry_hash = entry.get("hash")
                        ts = entry.get("ts")
                        if entry_hash:
                            # ts ì—†ìœ¼ë©´ ë¡œë“œ ì‹œì ìœ¼ë¡œ ëŒ€ì²´(ê¸°ì¡´ ìºì‹œ ë³´ì¡´)
                            self.seen_hashes[str(entry_hash)] = float(ts) if ts else now
                else:
                    # ì´ˆê¸° ë‹¨ê³„ ë ˆê±°ì‹œ í¬ë§·: hashes = ["...", "..."], ì¶”í›„ í•„ìš” ì—†ìœ¼ë©´ ì œê±°
                    for entry_hash in data.get("hashes", []):
                        if entry_hash:
                            self.seen_hashes[str(entry_hash)] = now

                logger.info(
                    f"[{spider_name}] Dedup Cache loaded: {len(self.seen_hashes)} entries"
                )
        except Exception as e:
            logger.warning(f"Cache load failed: {e}")
            self.seen_hashes = OrderedDict()

    def save_cache(self, spider_name):
        # ìºì‹œ íŒŒì¼ì— í•´ì‹œ ì €ì¥(JSON + ìƒí•œ ìœ ì§€)
        if not self.cache_file:
            return

        # ì €ì¥ ì§ì „ì— ìƒí•œì„ ë‹¤ì‹œ ì ìš©
        self._prune_cache()

        try:
            entries = [
                {"hash": entry_hash, "ts": int(ts)}
                for entry_hash, ts in self.seen_hashes.items()
            ]
            with open(self.cache_file, "w", encoding="utf-8") as f:
                json.dump({"version": 2, "entries": entries}, f, indent=2)
            logger.info(
                f"[{spider_name}] Dedup Cache saved",
                cache_total=len(self.seen_hashes),
                seen_this_run=len(self._seen_this_run),
            )
        except Exception as e:
            logger.warning(f"Cache save failed: {e}")

    def _hash_exists(self, item_hash: str) -> bool:
        return item_hash in self.seen_hashes

    def _add_hash(self, item_hash: str):
        # ì‚½ì… ì‹œê° ê¸°ë¡(ìˆœì„œ ìœ ì§€)
        self.seen_hashes[item_hash] = time.time()
        self._prune_cache()

    def _prune_cache(self):
        # ìƒí•œ ì¡°ê±´ì— ë”°ë¼ ì˜¤ë˜ëœ í•´ì‹œ ì œê±°
        if not self.seen_hashes:
            return

        now = time.time()

        # ê¸°ê°„ ê¸°ì¤€ ì •ë¦¬
        if self.max_days and self.max_days > 0:
            cutoff = now - (self.max_days * 86400)
            while self.seen_hashes:
                _, ts = next(iter(self.seen_hashes.items()))
                if ts >= cutoff:
                    break
                self.seen_hashes.popitem(last=False)

        # ê°œìˆ˜ ê¸°ì¤€ ì •ë¦¬
        if self.max_entries and self.max_entries > 0:
            while len(self.seen_hashes) > self.max_entries:
                self.seen_hashes.popitem(last=False)

    def _prune_unseen_entries(self):
        # ì´ë²ˆ ì‹¤í–‰ì—ì„œ ë³´ì´ì§€ ì•Šì€ ìºì‹œë¥¼ ì œê±°
        for entry_hash in list(self.seen_hashes.keys()):
            if entry_hash not in self._seen_this_run:
                self.seen_hashes.pop(entry_hash, None)

    def _log_summary(self, spider_name):
        # ì‹¤í–‰ ìš”ì•½ì„ ë¡œê·¸ì— ë‚¨ê¹€
        logger.info(
            f"[{spider_name}] Dedup summary",
            total=self.total_items,
            new=self.new_items,
            duplicates=self.duplicate_items,
        )

    def _notify_no_new(self, spider_name):
        # ì¤‘ë³µìœ¼ë¡œ ì¸í•´ ìƒˆ ë°ì´í„°ê°€ ì—†ì„ ë•Œ ì•Œë¦¼ ì „ì†¡
        if not self._notify_on_no_new:
            return

        # ì‹¤ì œ ë°ì´í„°ê°€ í•˜ë‚˜ë„ ì—†ì—ˆë˜ ê²½ìš°ëŠ” ì œì™¸
        if self.total_items == 0:
            return

        # ìƒˆ ë°ì´í„°ê°€ ì—†ê³ , ì¤‘ë³µë§Œ ë°œìƒí–ˆì„ ë•Œë§Œ ì•Œë¦¼
        if self.new_items != 0 or self.duplicate_items == 0:
            return

        if not self._webhook_url:
            logger.info(f"[{spider_name}] Webhook ë¯¸ì„¤ì •, ì¤‘ë³µ ì•Œë¦¼ ìŠ¤í‚µ")
            return

        payload = {
            "content": (
                f"ğŸ•·ï¸ {spider_name}: ì‹ ê·œ ë°ì´í„° ì—†ìŒ (ì¤‘ë³µ {self.duplicate_items}ê±´)."
            )
        }

        try:
            response = requests.post(
                self._webhook_url,
                json=payload,
                headers={"Content-Type": "application/json"},
                timeout=10,
            )
            if response.status_code in (200, 204):
                logger.info(f"[{spider_name}] ì‹ ê·œ ë°ì´í„° ì—†ìŒ ì•Œë¦¼ ì „ì†¡")
            else:
                logger.warning(
                    f"[{spider_name}] ì‹ ê·œ ë°ì´í„° ì—†ìŒ ì•Œë¦¼ ì‹¤íŒ¨",
                    status=response.status_code,
                )
        except Exception as e:
            logger.warning(f"[{spider_name}] ì‹ ê·œ ë°ì´í„° ì—†ìŒ ì•Œë¦¼ ì—ëŸ¬: {e}")

    def _resolve_spider(self, spider):
        if spider is not None:
            self._spider = spider
            return spider
        if self._spider is not None:
            return self._spider
        if self._crawler is not None:
            return getattr(self._crawler, "spider", None)
        return None
