"""
ì¤‘ë³µ ê²Œì‹œë¬¼ í•„í„°ë§ íŒŒì´í”„ë¼ì¸ (Supabase ê¸°ë°˜)
ë¡œì»¬ íŒŒì¼ ëŒ€ì‹  Supabase DBì— ìˆëŠ” dedup_idë¥¼ ì¡°íšŒí•˜ì—¬ ì¤‘ë³µì„ íŒë‹¨í•¨.
ë¶„ì‚° í™˜ê²½(íŒ€ í”„ë¡œì íŠ¸)ì—ì„œ ì—¬ëŸ¬ í¬ë¡¤ëŸ¬ê°€ ìƒíƒœë¥¼ ê³µìœ í•˜ê¸° ìœ„í•¨.
"""
import hashlib
import time
import os
import requests
from pathlib import Path
from scrapy.exceptions import DropItem
from supabase import create_client, Client
import structlog
from dotenv import load_dotenv

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ (Scrapy ë‹¨ë… ì‹¤í–‰ ì‹œ ëŒ€ë¹„)
load_dotenv()

logger = structlog.get_logger(__name__)


class DeduplicationPipeline:
    """
    Supabase ê¸°ë°˜ ì¤‘ë³µ í•„í„°ë§
    1. ìŠ¤íŒŒì´ë” ì‹œì‘ ì‹œ: DBì—ì„œ ìµœê·¼ Nê°œì˜ dedup_idë¥¼ ê°€ì ¸ì™€ ë©”ëª¨ë¦¬ì— ì ì¬ (Initial Load)
    2. ì•„ì´í…œ ì²˜ë¦¬ ì‹œ: ë©”ëª¨ë¦¬ì— ìˆìœ¼ë©´ Drop, ì—†ìœ¼ë©´ í†µê³¼
    (ë©”ëª¨ë¦¬ ì¶”ê°€ëŠ” process_item ì„±ê³µ ì‹œ)
    """

    def __init__(self, max_entries: int):
        self.max_entries = max_entries
        
        # ë©”ëª¨ë¦¬ ìƒì˜ ì¤‘ë³µ ì²´í¬ìš© Set
        self.seen_hashes = set()
        
        # Supabase í´ë¼ì´ì–¸íŠ¸
        self.supabase: Client = None
        
        # ìŠ¤íŒŒì´ë”/í¬ë¡¤ëŸ¬ ì°¸ì¡°
        self._crawler = None
        self._spider = None

        # ì•Œë¦¼ ì„¤ì •
        self._webhook_url = None
        self._notify_on_no_new = True

        # í†µê³„
        self.total_items = 0
        self.new_items = 0
        self.duplicate_items = 0

    @classmethod
    def from_crawler(cls, crawler):
        """Scrapy ì„¤ì •ê°’ì„ ì½ì–´ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”."""
        max_entries = crawler.settings.getint("DEDUP_MAX_ENTRIES", 20000)
        
        pipeline = cls(max_entries)
        pipeline._crawler = crawler
        pipeline._webhook_url = crawler.settings.get("DISCORD_WEBHOOK_URL")
        pipeline._notify_on_no_new = crawler.settings.getbool("NOTIFY_ON_NO_NEW_DATA", True)
        return pipeline

    def open_spider(self, spider=None):
        """ìŠ¤íŒŒì´ë” ì‹œì‘ ì‹œ DB ë™ê¸°í™”."""
        spider_obj = self._resolve_spider(spider)
        spider_name = spider_obj.name if spider_obj else "unknown"

        # í†µê³„ ì´ˆê¸°í™”
        self.total_items = 0
        self.new_items = 0
        self.duplicate_items = 0
        self.seen_hashes = set()

        # Supabase ì—°ê²°
        url = os.environ.get("SUPABASE_URL")
        key = os.environ.get("SUPABASE_KEY")

        if not url or not key:
            logger.warning("âš ï¸ Supabase ìê²©ì¦ëª…ì´ ì—†ì–´ ì¤‘ë³µ ì²´í¬ê°€ 'ë©”ëª¨ë¦¬ ì „ìš©(ì´ë²ˆ ì‹¤í–‰ë§Œ)'ìœ¼ë¡œ ë™ì‘í•©ë‹ˆë‹¤.")
            return

        try:
            self.supabase = create_client(url, key)
            logger.info(f"[{spider_name}] Supabase ì—°ê²° ì„±ê³µ. ì¤‘ë³µ ID ë™ê¸°í™” ì¤‘...")
            
            # ìµœê·¼ ì €ì¥ëœ ë°ì´í„°ì˜ dedup_idë§Œ ê°€ì ¸ì˜´ (ê°€ë²¼ìš´ ì¿¼ë¦¬)
            # limitì„ ì„¤ì •í•˜ì—¬ ë©”ëª¨ë¦¬ ê³¼ë¶€í•˜ ë°©ì§€
            res = self.supabase.table("darkweb_leaks")\
                .select("dedup_id")\
                .order("crawled_at", desc=True)\
                .limit(self.max_entries)\
                .execute()
            
            if res.data:
                count = 0
                for row in res.data:
                    did = row.get("dedup_id")
                    if did:
                        self.seen_hashes.add(did)
                        count += 1
                
                # Statsì— ì €ì¥ (RichProgressì—ì„œ ì½ì„ ìˆ˜ ìˆë„ë¡)
                if self._crawler:
                    self._crawler.stats.set_value("dedup/loaded_ids", count)
                
                logger.info(f"[{spider_name}] âœ… Supabaseì—ì„œ {count}ê°œì˜ ì¤‘ë³µ ID ë¡œë“œ ì™„ë£Œ.")
            else:
                if self._crawler:
                    self._crawler.stats.set_value("dedup/loaded_ids", 0)
                logger.info(f"[{spider_name}] DBê°€ ë¹„ì–´ìˆê±°ë‚˜ ì´ˆê¸° ìƒíƒœì…ë‹ˆë‹¤.")

        except Exception as e:
            logger.error(f"âŒ Supabase ì´ˆê¸° ë¡œë“œ ì‹¤íŒ¨: {e}")

        # [Optimization] ìŠ¤íŒŒì´ë”ì—ê²Œ ì¤‘ë³µ ID ì„¸íŠ¸ ì£¼ì… (Pre-request Filteringìš©)
        # ìŠ¤íŒŒì´ë”ê°€ ìš”ì²­ì„ ë³´ë‚´ê¸° ì „ì— ì´ ì„¸íŠ¸ë¥¼ í™•ì¸í•˜ì—¬ ë¶ˆí•„ìš”í•œ IOë¥¼ ì¤„ì¼ ìˆ˜ ìˆìŒ
        if spider_obj:
            spider_obj.seen_ids = self.seen_hashes
            logger.info(f"[{spider_name}] ğŸš€ Pre-filtering Activated: {len(self.seen_hashes)} IDs injected into spider.")

    def get_hash(self, item):
        """ê²Œì‹œë¬¼ ê³ ìœ  í•´ì‹œ ìƒì„± (ë˜ëŠ” ê¸°ì¡´ ID ì‚¬ìš©)"""
        custom_id = item.get("dedup_id")
        if custom_id:
            return custom_id

        title = item.get("title", "")
        author = item.get("author", "")
        key = f"{title}|{author}"
        gen_hash = hashlib.md5(key.encode()).hexdigest()
        
        # ìƒì„±ëœ í•´ì‹œë¥¼ ì•„ì´í…œì— ê¸°ë¡ (ë‹¤ìŒ íŒŒì´í”„ë¼ì¸ì¸ SupabasePipeline ë“±ì—ì„œ ì‚¬ìš©)
        item["dedup_id"] = gen_hash
        return gen_hash

    def process_item(self, item, spider=None):
        """ì¤‘ë³µ ê²€ì‚¬"""
        item_hash = self.get_hash(item)
        self.total_items += 1

        if item_hash in self.seen_hashes:
            self.duplicate_items += 1
            # ë¡œê·¸ ë ˆë²¨ ì¡°ì • (ë„ˆë¬´ ì‹œë„ëŸ¬ìš°ë©´ debugë¡œ ë³€ê²½)
            logger.debug(f"Duplicate (DB): {item.get('title', '')[:30]}")
            raise DropItem(f"Duplicate (DB): {item.get('title', '')[:30]}")

        # ìƒˆë¡œìš´ ì•„ì´í…œ -> ë©”ëª¨ë¦¬ì— ì¶”ê°€ (DB ì €ì¥ì€ í›„ì† SupabasePipelineì´ ë‹´ë‹¹)
        self.new_items += 1
        self.seen_hashes.add(item_hash)
        return item

    def close_spider(self, spider=None):
        """ì¢…ë£Œ ì‹œ ì•Œë¦¼."""
        spider_obj = self._resolve_spider(spider)
        spider_name = spider_obj.name if spider_obj else "unknown"

        self._log_summary(spider_name)
        self._notify_no_new(spider_name)

    def _log_summary(self, spider_name):
        logger.info(
            f"[{spider_name}] Dedup summary (Supabase Sync)",
            total=self.total_items,
            new=self.new_items,
            duplicates=self.duplicate_items,
        )

    def _notify_no_new(self, spider_name):
        """ì‹ ê·œ ë°ì´í„°ê°€ ì—†ì„ ë•Œ ì•Œë¦¼."""
        if not self._notify_on_no_new:
            return
        
        # ì „ì²´ ì•„ì´í…œì´ 0ê°œë©´(í¬ë¡¤ë§ ì‹¤íŒ¨ ë“±) ì•Œë¦¼ ì•ˆ í•¨
        if self.total_items == 0:
            return

        # ì‹ ê·œ ë°ì´í„°ê°€ í•˜ë‚˜ë¼ë„ ìˆìœ¼ë©´ ì•Œë¦¼ ì•ˆ í•¨ (DiscordNotifyPipelineì´ ê°œë³„ ì•Œë¦¼ ë³´ë‚´ë¯€ë¡œ)
        if self.new_items != 0:
            return
            
        # ì¤‘ë³µë§Œ 100%ì¼ ë•Œ ì•Œë¦¼
        if not self._webhook_url:
            return

        payload = {
            "content": (
                f"ğŸ•·ï¸ **{spider_name}**: ì‹ ê·œ ë°ì´í„° ì—†ìŒ (DB ì¤‘ë³µ {self.duplicate_items}ê±´ í™•ì¸)."
            )
        }

        try:
            # íƒ€ì„ì•„ì›ƒ ì§§ê²Œ ì„¤ì •
            requests.post(
                self._webhook_url,
                json=payload,
                headers={"Content-Type": "application/json"},
                timeout=5
            )
            # logger.info(f"[{spider_name}] 'ì‹ ê·œ ì—†ìŒ' ì•Œë¦¼ ì „ì†¡ ì™„ë£Œ")
        except Exception:
            pass # ì•Œë¦¼ ì‹¤íŒ¨ëŠ” ì¡°ìš©íˆ ë„˜ì–´ê°

    def _resolve_spider(self, spider):
        if spider is not None:
            self._spider = spider
            return spider
        if self._spider is not None:
            return self._spider
        if self._crawler is not None:
            return getattr(self._crawler, "spider", None)
        return None
