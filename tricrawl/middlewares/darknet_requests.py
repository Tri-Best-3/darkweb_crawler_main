"""
Scrapyì˜ Twisted Reactor ëŒ€ì‹  requests ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ìš´ë¡œë“œ
í¬ëŸ¼ êµ¬ì¡°ì—ì„œ SOCKS5 í”„ë¡ì‹œ ì—°ê²°ì— ë¬¸ì œê°€ ìˆì–´ì„œ ëŒ€ì²´ êµ¬í˜„
"""
import asyncio
import requests
import structlog
from scrapy.http import HtmlResponse
# from scrapy.downloadermiddlewares.retry import RetryMiddleware
from scrapy.utils.python import to_bytes

logger = structlog.get_logger(__name__)

class RequestsDownloaderMiddleware:
    """
    .onion ìš”ì²­ì„ requests + socks5hë¡œ ì²˜ë¦¬í•˜ëŠ” ëŒ€ì²´ ë‹¤ìš´ë¡œë“œ ë¯¸ë“¤ì›¨ì–´.

    ëª©ì :
    - Scrapy ê¸°ë³¸ ë‹¤ìš´ë¡œë”ì˜ socks ë¯¸ì§€ì›/ë¶ˆì•ˆì • ë¬¸ì œ íšŒí”¼
    - .onionë§Œ ê°€ë¡œì±„ê³  ë‚˜ë¨¸ì§€ëŠ” Scrapy ê¸°ë³¸ íë¦„ ìœ ì§€
    """
    def __init__(self, tor_proxy, settings):
        self.tor_proxy = tor_proxy
        self.settings = settings
        # requestsìš© í”„ë¡ì‹œ ë”•ì…”ë„ˆë¦¬ (socks5hëŠ” DNS í•´ì„ë„ Torì—ì„œ ìˆ˜í–‰)
        self.proxies = {
            'http': f"socks5h://{tor_proxy}",
            'https': f"socks5h://{tor_proxy}"
        } if tor_proxy else None

    @classmethod
    def from_crawler(cls, crawler):
        """TOR_PROXY_HOST/PORTë¥¼ í•©ì³ requests í”„ë¡ì‹œ ë¬¸ìì—´ì„ êµ¬ì„±."""
        # TOR_PROXYHOST, TOR_PROXYPORT ê°€ì ¸ì˜¤ê¸°
        host = crawler.settings.get("TOR_PROXY_HOST", "127.0.0.1")
        port = crawler.settings.get("TOR_PROXY_PORT", "9050")
        tor_proxy = f"{host}:{port}"
        return cls(tor_proxy, crawler.settings)

    async def process_request(self, request, spider=None):
        """ìš”ì²­ì„ ê°€ë¡œì±„ .onionì´ë©´ async threadë¡œ ë‹¤ìš´ë¡œë“œ."""
        # .onion ì£¼ì†Œì— ëŒ€í•´ì„œë§Œ custom downloader ì‚¬ìš©
        if ".onion" not in request.url:
             return None # Scrapy ê¸°ë³¸ ì²˜ë¦¬

        logger.info("Requests ë‹¤ìš´ë¡œë” ì‚¬ìš©(Async)", url=request.url)

        # asyncio threadë¡œ ì‹¤í–‰í•´ Deferred ë°˜í™˜ ë¬¸ì œ í”¼í•¨
        return await asyncio.to_thread(self._download, request, spider)

    def _download(self, request, spider):
        """ì‹¤ì œ ë‹¤ìš´ë¡œë“œ ë¡œì§(Thread ì‹¤í–‰)."""
        settings = spider.settings if spider is not None else self.settings
        
        # 1. Scrapy Request Header -> requests Header ë³€í™˜
        # (CookiesMiddlewareê°€ ì±„ì›Œì¤€ Cookie í—¤ë”ë„ ì—¬ê¸°ì„œ ë„˜ì–´ê°)
        # requests.headersëŠ” dictë¥¼ ê¸°ëŒ€í•¨
        req_headers = {}
        if request.headers:
            for k, v in request.headers.items():
                # Scrapy í—¤ë”ëŠ” bytes ë¦¬ìŠ¤íŠ¸ í˜•íƒœ
                key_str = to_bytes(k).decode('latin1')
                val_str = to_bytes(v[0]).decode('latin1')
                req_headers[key_str] = val_str

        # User-Agent ë³´ê°•
        if "User-Agent" not in req_headers:
             req_headers["User-Agent"] = settings.get("USER_AGENT")

        try:
            resp = requests.get(
                request.url,
                proxies=self.proxies,
                timeout=int(settings.get('DOWNLOAD_TIMEOUT', 60)),
                verify=settings.getbool("VERIFY_SSL", True),
                headers=req_headers
            )

            # 2. requests Header -> Scrapy Response Header ë³€í™˜
            # (Set-Cookie ë“± ì‘ë‹µ í—¤ë”ë¥¼ Scrapyë¡œ ì „ë‹¬)
            # ì£¼ì˜: requestsëŠ” ì´ë¯¸ contentë¥¼ ë””ì½”ë”©í–ˆìœ¼ë¯€ë¡œ, 
            # 'Content-Encoding: gzip' í—¤ë”ê°€ ë‚¨ì•„ìˆìœ¼ë©´ Scrapyê°€ ë˜ ë””ì½”ë”©ì„ ì‹œë„í•˜ë‹¤ ì—ëŸ¬ë‚¨(Not a gzipped file).
            # ë”°ë¼ì„œ Content-Encoding, Content-LengthëŠ” ì œê±°í•˜ê³  ë„˜ê²¨ì•¼ í•¨.
            resp_headers = {}
            for k, v in resp.headers.items():
                if k.lower() in ['content-encoding', 'content-length']:
                    continue
                resp_headers[k] = v
            
            return HtmlResponse(
                url=request.url,
                status=resp.status_code,
                body=resp.content,
                encoding=resp.encoding or 'utf-8',
                request=request,
                headers=resp_headers
            )

        except Exception as e:
            # ì—ëŸ¬ ë©”ì‹œì§€ë¥¼ ì‚¬ëŒì´ ì½ê¸° ì‰½ê²Œ ë³€í™˜
            error_str = str(e)
            if "RemoteDisconnected" in error_str or "closed connection" in error_str:
                friendly_msg = "ğŸ”„ Tor ì—°ê²° ëŠê¹€ (ì¬ì‹œë„ ì˜ˆì •)"
            elif "timed out" in error_str.lower() or "timeout" in error_str.lower():
                friendly_msg = "â±ï¸ ì—°ê²° ì‹œê°„ ì´ˆê³¼ (ì¬ì‹œë„ ì˜ˆì •)"
            elif "refused" in error_str.lower():
                friendly_msg = "ğŸš« ì—°ê²° ê±°ë¶€ë¨"
            elif "reset" in error_str.lower():
                friendly_msg = "ğŸ”„ ì—°ê²° ë¦¬ì…‹ë¨ (ì¬ì‹œë„ ì˜ˆì •)"
            else:
                friendly_msg = f"âŒ ì—°ê²° ì‹¤íŒ¨: {error_str[:50]}"
            
            logger.error(friendly_msg, url=request.url[:60])
            """
            ì—ëŸ¬ ë°œìƒ ì‹œ ì—¬ê¸°ì„œ Noneì„ ë¦¬í„´í•˜ë©´ ì•ˆ ë˜ê³ (Thread ì•ˆì´ë¯€ë¡œ),
            ì˜ˆì™¸ë¥¼ ë°œìƒì‹œí‚¤ê±°ë‚˜ Scrapy Response ê°ì²´ë¥¼ ë§Œë“¤ì–´ì•¼ í•˜ëŠ”ë°,
            ì¼ë°˜ì ìœ¼ë¡œëŠ” ì˜ˆì™¸ë¥¼ ë˜ì ¸ì„œ Scrapy ì—”ì§„ì´ ì—ëŸ¬ ì²˜ë¦¬(Retry ë“±)ë¥¼ í•˜ê²Œ ë‘ 
            """
            raise e
