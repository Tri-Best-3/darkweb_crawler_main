"""
Rich Progress Bar Extension

Scrapy í¬ë¡¤ë§ ì§„í–‰ ìƒí™©ì„ ì‹œê°ì ìœ¼ë¡œ í‘œì‹œí•˜ëŠ” í™•ì¥.
- ì‹¤ì‹œê°„ ì§„í–‰ë¥  í‘œì‹œ (Items, Requests, Errors)
- ì‹œì‘ ì‹œ ì„¤ì • ìƒíƒœ í‘œì‹œ (Discord, Supabase ë“±)
- ìµœê·¼ í¬ë¡¤ë§ ë°ì´í„° í•œ ì¤„ í‘œì‹œ
- ì™„ë£Œ ì‹œ ìµœì¢… í†µê³„ ì¶œë ¥
"""
import os
from rich.progress import Progress, SpinnerColumn, BarColumn, TextColumn, TimeElapsedColumn
from rich.console import Console, Group
from rich.panel import Panel
from rich.live import Live
from scrapy import signals
from scrapy.exceptions import NotConfigured


class RichProgress:
    """Scrapy í¬ë¡¤ë§ ì§„í–‰ ìƒí™©ì„ Rich Progress Barë¡œ í‘œì‹œ."""

    def __init__(self, crawler):
        self.crawler = crawler
        self.stats = crawler.stats
        self.console = Console()
        
        # Progress Bar ì„¤ì •
        self.progress = Progress(
            SpinnerColumn("dots"),  # ë” ë¶€ë“œëŸ¬ìš´ ìŠ¤í”¼ë„ˆ
            TextColumn("[bold cyan]{task.description}[/bold cyan]"),
            BarColumn(bar_width=25),
            TimeElapsedColumn(),
            TextColumn("{task.fields[info]}"),
            console=self.console,
            transient=False,
        )
        self.task_id = None
        
        # ìƒíƒœ í‘œì‹œìš©
        self.last_item_text = "[dim]ğŸ”§ ì´ˆê¸°í™” ì¤‘...[/dim]"
        self.first_response = False  # ì²« ì‘ë‹µ ì—¬ë¶€
        
        # Live ì»¨í…ìŠ¤íŠ¸
        self.live = None

    @classmethod
    def from_crawler(cls, crawler):
        if not crawler.settings.getbool("RICH_PROGRESS_ENABLED", True):
            raise NotConfigured

        ext = cls(crawler)
        crawler.signals.connect(ext.spider_opened, signal=signals.spider_opened)
        crawler.signals.connect(ext.spider_closed, signal=signals.spider_closed)
        crawler.signals.connect(ext.item_scraped, signal=signals.item_scraped)
        crawler.signals.connect(ext.item_dropped, signal=signals.item_dropped)
        crawler.signals.connect(ext.response_received, signal=signals.response_received)
        crawler.signals.connect(ext.request_scheduled, signal=signals.request_scheduled)
        return ext

    def _print_startup_status(self, spider):
        """ì‹œì‘ ì‹œ ì„¤ì • ìƒíƒœ ì¶œë ¥."""
        settings = self.crawler.settings
        
        # Discord ìƒíƒœ
        discord_url = settings.get("DISCORD_WEBHOOK_URL")
        discord_status = "[green]âœ“ ì—°ê²°ë¨[/green]" if discord_url else "[yellow]âš  ë¯¸ì„¤ì •[/yellow]"
        
        # Supabase ìƒíƒœ
        supabase_url = os.getenv("SUPABASE_URL")
        supabase_status = "[green]âœ“ ì—°ê²°ë¨[/green]" if supabase_url else "[red]âœ— ë¯¸ì„¤ì •[/red]"
        
        # ì¤‘ë³µ ID ë¡œë“œ ê°œìˆ˜ (dedup pipelineì—ì„œ ì„¤ì •)
        loaded_ids = self.stats.get_value("dedup/loaded_ids", 0)
        dedup_status = f"[cyan]{loaded_ids:,}[/cyan]ê°œ" if loaded_ids else "[dim]0ê°œ[/dim]"
        
        # Log íŒŒì¼ (ê²½ë¡œ ì¶•ì•½)
        log_file = settings.get("LOG_FILE", "")
        if log_file:
            log_file_display = "..." + str(log_file)[-35:] if len(str(log_file)) > 35 else str(log_file)
        else:
            log_file_display = "[dim]ì—†ìŒ[/dim]"
        
        status_lines = [
            f"ğŸ•·ï¸  [bold]ìŠ¤íŒŒì´ë”:[/bold] {spider.name}",
            f"ğŸ“¢  [bold]ë””ìŠ¤ì½”ë“œ ì•Œë¦¼:[/bold] {discord_status}",
            f"ğŸ’¾  [bold]Supabase DB:[/bold] {supabase_status}",
            f"ğŸ”  [bold]ì¤‘ë³µ ID ë¡œë“œ:[/bold] {dedup_status}",
            f"ğŸ“  [bold]ë¡œê·¸ íŒŒì¼:[/bold] {log_file_display}",
        ]
        
        self.console.print(Panel(
            "\n".join(status_lines),
            title="[bold blue]ğŸš€ Start Crawling[/bold blue]",
            border_style="blue",
            padding=(0, 1),
            width=50,
        ))

    def _build_display(self):
        """Progress Bar + ìµœê·¼ ì•„ì´í…œì„ í•©ì¹œ í‘œì‹œ ê·¸ë£¹ ìƒì„±."""
        from rich.text import Text
        return Group(
            self.progress,
            Text.from_markup(f"  {self.last_item_text}"),
        )

    def spider_opened(self, spider):
        """ìŠ¤íŒŒì´ë” ì‹œì‘ ì‹œ ìƒíƒœ í‘œì‹œ ë° Progress Bar ì‹œì‘."""
        self._print_startup_status(spider)
        self.console.print()  # ë¹ˆ ì¤„
        
        self.task_id = self.progress.add_task(
            "Crawling",
            total=None,
            info="[dim]Initializing...[/dim]"
        )
        
        # Live ì»¨í…ìŠ¤íŠ¸ ì‹œì‘ (ë¶€ë“œëŸ¬ìš´ ì—…ë°ì´íŠ¸)
        self.live = Live(
            self._build_display(),
            console=self.console,
            refresh_per_second=10,  # ì´ˆë‹¹ 10íšŒ ì—…ë°ì´íŠ¸
            transient=False,
        )
        self.live.start()

    def spider_closed(self, spider):
        """ìŠ¤íŒŒì´ë” ì¢…ë£Œ ì‹œ Progress Bar ì •ì§€ ë° ìµœì¢… í†µê³„ ì¶œë ¥."""
        if self.live:
            self.live.stop()
        
        scraped = self.stats.get_value("item_scraped_count", 0)
        dropped = self.stats.get_value("item_dropped_count", 0)
        req_count = self.stats.get_value("downloader/request_count", 0)
        resp_count = self.stats.get_value("downloader/response_count", 0)
        err_count = self.stats.get_value("log_count/ERROR", 0)
        
        result_lines = [
            f"ğŸ“¦  [bold]ìˆ˜ì§‘:[/bold] [bold green]{scraped}[/bold green]ê±´",
            f"ğŸ—‘ï¸   [bold]ì¤‘ë³µ/í•„í„°:[/bold] {dropped}ê±´",
            f"ğŸŒ  [bold]ìš”ì²­/ì‘ë‹µ:[/bold] {req_count}/{resp_count}",
            f"âŒ  [bold]ì—ëŸ¬:[/bold] [bold red]{err_count}[/bold red]ê±´",
        ]
        
        self.console.print()
        self.console.print(Panel(
            "\n".join(result_lines),
            title="[bold green]âœ¨ Crawling Completed[/bold green]",
            border_style="green",
            padding=(0, 1),
            width=40,
        ))

    def request_scheduled(self, request, spider):
        """ìš”ì²­ ìŠ¤ì¼€ì¤„ ì‹œ ìƒíƒœ í‘œì‹œ (ì²« ìš”ì²­ = Tor ì—°ê²° ì¤‘)."""
        if not self.first_response:
            self.last_item_text = "[yellow]ğŸŒ Tor ì—°ê²° ì¤‘...[/yellow]"
            if self.live:
                self.live.update(self._build_display())

    def item_scraped(self, item, spider):
        """ì•„ì´í…œ ìŠ¤í¬ë© ì‹œ ìƒíƒœ ì—…ë°ì´íŠ¸."""
        title = item.get("title", "")[:30]
        self.last_item_text = f"[cyan]â³ í¬ë¡¤ë§ ì¤‘[/cyan] | [green]âœ… ìˆ˜ì§‘: {title}[/green]"
        self._update_status()

    def item_dropped(self, item, response, exception, spider):
        """ì•„ì´í…œ ë“œë¡­ ì‹œ ìƒíƒœ ì—…ë°ì´íŠ¸."""
        title = item.get("title", "")[:30] if hasattr(item, "get") else str(item)[:30]
        self.last_item_text = f"[cyan]â³ í¬ë¡¤ë§ ì¤‘[/cyan] | [dim]ğŸ”„ ìŠ¤í‚µ: {title}[/dim]"
        self._update_status()

    def response_received(self, response, request, spider):
        """ì‘ë‹µ ìˆ˜ì‹  ì‹œ ìƒíƒœ ì—…ë°ì´íŠ¸."""
        if not self.first_response:
            self.first_response = True
            self.last_item_text = "[cyan]â³ í¬ë¡¤ë§ ì¤‘...[/cyan]"
        self._update_status()

    def _update_status(self):
        """Progress Bar ìƒíƒœ í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸."""
        if self.task_id is None:
            return
            
        scraped = self.stats.get_value("item_scraped_count", 0)
        dropped = self.stats.get_value("item_dropped_count", 0)
        req_count = self.stats.get_value("downloader/request_count", 0)
        err_count = self.stats.get_value("log_count/ERROR", 0)

        info_text = (
            f"ğŸ“¦ [green]{scraped}[/green] | "
            f"ğŸ—‘ï¸ {dropped} | "
            f"ğŸŒ {req_count} | "
            f"âŒ [red]{err_count}[/red]"
        )

        self.progress.update(self.task_id, info=info_text)
        
        # Live ì»¨í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸
        if self.live:
            self.live.update(self._build_display())

