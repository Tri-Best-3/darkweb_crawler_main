"""
DarkNetArmy Forum Spider
Target: http://dna777qhcrxy5sbvk7rkdd2phhxbftpdtxvwibih26nr275cdazx4uyd.onion/
Type: XenForo Forum
"""
import scrapy
import structlog
import yaml
from pathlib import Path
from tricrawl.items import LeakItem
from datetime import datetime, timedelta, timezone

logger = structlog.get_logger(__name__)


class DarkNetArmySpider(scrapy.Spider):
    """
    DarkNetArmy í¬ëŸ¼ í¬ë¡¤ëŸ¬ (XenForo).

    ë°ì´í„° ì»¨íŠ¸ë™íŠ¸:
    - LeakItemì˜ í•„ìˆ˜ í•„ë“œ(source/title/url/author/timestamp)ë¥¼ ë°˜ë“œì‹œ ì±„ì›€
    - contentëŠ” ìš”ì•½/í´ë¦° í…ìŠ¤íŠ¸ë¡œ êµ¬ì„± (í‚¤ì›Œë“œ í•„í„° ì…ë ¥)
    - categoryëŠ” ê°€ëŠ¥í•˜ë©´ ê²Œì‹œíŒ/ë¶„ë¥˜ëª…ìœ¼ë¡œ ì±„ì›€
    """
    
    name = "darknet_army"
    
    # ë™ì  URL ìƒì„± (Config ë¡œë“œ í›„ __init__ì—ì„œ ì„¤ì •)
    start_urls = []
    
    custom_settings = {
        'ROBOTSTXT_OBEY': False,
        'DOWNLOAD_TIMEOUT': 120,
        'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; rv:109.0) Gecko/20100101 Firefox/115.0',
        'COOKIES_ENABLED': True,
        # DarkNet ì „ìš© ë¯¸ë“¤ì›¨ì–´ ì‚¬ìš©
        'DOWNLOADER_MIDDLEWARES': {
            'tricrawl.middlewares.darknet_requests.RequestsDownloaderMiddleware': 543,
            'tricrawl.middlewares.TorProxyMiddleware': None,
            'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware': None,
        }
    }
    
    def __init__(self, *args, **kwargs):
        """YAML ì„¤ì •ì„ ë¡œë“œí•˜ê³  start_urls/board limitsë¥¼ êµ¬ì„±í•œë‹¤."""
        super(DarkNetArmySpider, self).__init__(*args, **kwargs)
        
        # ì„¤ì • íŒŒì¼ ë¡œë“œ
        self.config = {}
        try:
            # í”„ë¡œì íŠ¸ ë£¨íŠ¸ (tricrawl/spiders -> ../../)
            project_root = Path(__file__).resolve().parents[2]
            config_path = project_root / "config" / "crawler_config.yaml"
            
            if config_path.exists():
                with open(config_path, "r", encoding="utf-8") as f:
                    self.config = yaml.safe_load(f) or {}
                logger.info(f"Config loaded from {config_path}")
            else:
                logger.warning("Config file not found, using defaults")
                
        except Exception as e:
            logger.error(f"Config load failed: {e}")
            
        # ì „ì—­ ì„¤ì • ì ìš©
        global_conf = self.config.get('global', {})
        self.days_limit = global_conf.get('days_to_crawl', 3)
        override_days = kwargs.get("days_limit")
        if override_days is not None:
            try:
                self.days_limit = int(override_days)
            except ValueError:
                logger.warning("Invalid days_limit override", value=override_days)
        
        # ìŠ¤íŒŒì´ë”ë³„ ì„¤ì • ë¡œë“œ ë° start_urls êµ¬ì„±
        spider_conf = self.config.get('spiders', {}).get('darknet_army', {})
        self.target_url = spider_conf.get('target_url')
        self.endpoints = spider_conf.get('endpoints', {})
        self.board_limits = spider_conf.get('boards', {})
        
        if self.target_url and self.endpoints:
            # Base URLì˜ trailing slash ì²˜ë¦¬
            base = self.target_url.rstrip('/')
            for key, path in self.endpoints.items():
                # Endpoints pathì˜ leading slash ì²˜ë¦¬
                clean_path = path.lstrip('/')
                full_url = f"{base}/{clean_path}"
                self.start_urls.append(full_url)
                logger.debug(f"Added start URL: {full_url} (Key: {key})")
        else:
            logger.error("Target URL or Endpoints NOT found in config. Spider may not crawl anything.")

        # ê¸°ë³¸ê°’ ë¡œì§ ì¡°ì •: Configì— ì—†ìœ¼ë©´ ë‚´ë¶€ ê¸°ë³¸ê°’(5) ì‚¬ìš©
        self.default_max_pages = 5
        
        logger.info(f"Loaded Config - Global Days: {self.days_limit}, URLs: {len(self.start_urls)}")

    def get_max_pages_for_url(self, url):
        """URLì— í•´ë‹¹í•˜ëŠ” ê²Œì‹œíŒë³„ ì œí•œ í™•ì¸ (configì˜ boards ê¸°ì¤€)."""
        # Configì˜ Endpoints ê²½ë¡œë¥¼ ì—­ì¶”ì í•˜ì—¬ Keyë¥¼ ì°¾ê³ , ê·¸ Keyë¡œ Limitsë¥¼ ì¡°íšŒ
        # í˜„ì¬ëŠ” URLì— endpoint pathê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ ë‹¨ìˆœ ë¬¸ìì—´ ë§¤ì¹­ìœ¼ë¡œ í™•ì¸
        for key, path in self.endpoints.items():
            # URL ë””ì½”ë”© ë¬¸ì œê°€ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë‹¨ìˆœ í¬í•¨ ì—¬ë¶€ í™•ì¸ì´ ì•ˆì „
            # Configì˜ path ë¶€ë¶„ë§Œ ì˜ë¼ì„œ ë¹„êµ
            if path.lstrip('/') in url:
                return self.board_limits.get(key, self.default_max_pages)
        
        return self.default_max_pages

    def parse(self, response):
        """
        í¬ëŸ¼ ëª©ë¡(Latest posts ë“±) íŒŒì‹± - XenForo List View.

        - ë¦¬ìŠ¤íŠ¸ì—ì„œ ì œëª©/ì‘ì„±ì/ì‹œê°„/ë§í¬ë§Œ ì¶”ì¶œ
        - ë‚ ì§œ ì»·ì˜¤í”„ ë¡œì§ìœ¼ë¡œ ë¶ˆí•„ìš”í•œ í˜ì´ì§€ë„¤ì´ì…˜ì„ ì¤„ì„
        """
        # í˜„ì¬ í˜ì´ì§€ ì¹´ìš´íŠ¸ (ê¸°ë³¸ 1)
        page_count = response.meta.get('page_count', 1)
        
        # í˜„ì¬ ê²Œì‹œíŒì˜ Max Pages ê²°ì •
        current_max_pages = self.get_max_pages_for_url(response.url)
        
        logger.info(f"DarkNetArmy ì ‘ì† (Page {page_count}/{current_max_pages})", url=response.url)
        
        # ... (ì¤‘ëµ) ... --> ê¸°ì¡´ ì½”ë“œ ìœ ì§€, cutoff_date ë¶€ë¶„ë§Œ ìˆ˜ì •
        
        # XenForo ê²Œì‹œë¬¼ ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œ (li.structItem or div.structItem)
        threads = response.css(".structItem")
        
        if not threads:
            logger.warning("ê²Œì‹œë¬¼ ëª©ë¡ ë°œê²¬ ì‹¤íŒ¨ (structItem), êµ¬ì¡° ë³€ê²½ ë˜ëŠ” ê¶Œí•œ ë¬¸ì œ ê°€ëŠ¥ì„±")
            return

        logger.info(f"ê²Œì‹œë¬¼ ëª©ë¡ {len(threads)}ê°œ ê°ì§€ (í•„í„°ë§ ì „)")
        
        found_recent_on_this_page = False
        found_old_normal_post = False  # ì¼ë°˜ ê²Œì‹œê¸€ ì¤‘ ì˜¤ë˜ëœ ê¸€ ë°œê²¬ ì—¬ë¶€
        
        # ê³µì§€ì‚¬í•­(Sticky)ë§Œ ìˆëŠ” í˜ì´ì§€ì¼ ê²½ìš°, ì¼ë°˜ ê²Œì‹œê¸€ì´ ì—†ìœ¼ë¯€ë¡œ ë‹¤ìŒ í˜ì´ì§€ë¥¼ ë´ì•¼ í•¨
        # ë”°ë¼ì„œ "ì¼ë°˜ ê²Œì‹œê¸€(Non-Sticky) ì¤‘ ìµœì‹ ê¸€ì´ ìˆëŠ”ì§€"ë¥¼ ì²´í¬í•˜ê±°ë‚˜,
        # "ì¼ë°˜ ê²Œì‹œê¸€ì´ í•˜ë‚˜ë„ ì—†ìœ¼ë©´" ë‹¤ìŒ í˜ì´ì§€ë¡œ ê°€ì•¼ í•¨.
        # ì „ëµ: "ì˜¤ë˜ëœ ê¸€"ì„ ë§Œë‚¬ì„ ë•Œ, ê·¸ê²ƒì´ Stickyë¼ë©´ ë¬´ì‹œí•˜ê³  ê³„ì† ì§„í–‰. 
        # Non-Stickyì¸ë° ì˜¤ë˜ëœ ê¸€ì´ë©´ -> ê·¸ ì‹œì ì—ì„œ ì¤‘ë‹¨ ê³ ë ¤.
        
        for thread in threads:
            # Sticky ì—¬ë¶€ í™•ì¸
            is_sticky = "structItem--status--sticky" in (thread.attrib.get("class") or "")
            
            # 1. ì œëª© ë° ìƒì„¸ ë§í¬ (Selector ë³´ê°•)
            # ì¼ë°˜ì ì¸ XenForo: .structItem-title a
            # ì¼ë¶€ í…Œë§ˆ: .structItem-cell--title a
            title_el = thread.css(".structItem-title a") or thread.css(".structItem-cell--title a")
            
            # ì—¬ê¸°ì„œë„ ì—†ìœ¼ë©´ data-preview-url ì†ì„±ì„ ê°€ì§„ a íƒœê·¸ ì°¾ê¸°
            if not title_el:
                title_el = thread.css("a[data-preview-url]")

            link = title_el.css("::attr(href)").get()
            title = (title_el.css("::text").get() or "").strip()
            
            # 2. ë©”íƒ€ë°ì´í„° (ì‘ì„±ì, ë‚ ì§œ)
            author = thread.css(".structItem-parts .username::text").get() or \
                     thread.css(".structItem-cell--avatar img::attr(alt)").get() or "Unknown"
            
            # ë‚ ì§œ: ê°€ëŠ¥í•œ timestamp ì¤‘ ìµœì‹ ê°’ì„ ì‚¬ìš© (ìµœê·¼ ì—…ë°ì´íŠ¸ ê¸°ì¤€)
            # data-time (Unix timestamp)ì´ ê°€ì¥ ì‹ ë¢°í•  ìˆ˜ ìˆìŒ
            timestamp_candidates = []
            for raw_ts in thread.css("time::attr(data-time)").getall():
                try:
                    ts = int(raw_ts)
                except (TypeError, ValueError):
                    continue
                # ms ë‹¨ìœ„ timestamp ë°©ì–´
                if ts > 10**12:
                    ts = ts // 1000
                timestamp_candidates.append(ts)

            timestamp_unix = max(timestamp_candidates) if timestamp_candidates else None

            # Fallback: datetime ì†ì„± (ISO) ì¤‘ ìµœì‹ ê°’ ì„ íƒ
            date_str_candidates = []
            if not timestamp_unix:
                date_str_candidates = thread.css("time::attr(datetime)").getall()

            date_time = None
            is_recent = False
            
            # ë‚ ì§œ íŒŒì‹± ë¡œì§ (Flag: Force UTC)
            try:
                dt = None
                if timestamp_unix:
                    dt = datetime.fromtimestamp(int(timestamp_unix), tz=timezone.utc)
                elif date_str_candidates:
                    parsed = []
                    for date_str in date_str_candidates:
                        try:
                            d = datetime.fromisoformat(date_str.replace('Z', '+00:00'))
                            if d.tzinfo is None:
                                d = d.replace(tzinfo=timezone.utc)
                            parsed.append(d)
                        except Exception:
                            continue
                    if parsed:
                        dt = max(parsed)
                
                if dt:
                    date_time = dt.isoformat()
                    # ì„¤ì •ëœ ë‚ ì§œ ì œí•œ ì ìš© (UTC ê¸°ì¤€ ë¹„êµ)
                    cutoff_date = datetime.now(timezone.utc) - timedelta(days=self.days_limit)
                    
                    if dt >= cutoff_date:
                        is_recent = True
                        # Stickyì—¬ë„ ìµœì‹ ì´ë©´ OK, Non-Stickyë©´ ë‹¹ì—°íˆ OK
                        found_recent_on_this_page = True
                    else:
                        # ì˜¤ë˜ëœ ê¸€ì„.
                        if is_sticky:
                            # StickyëŠ” ì˜¤ë˜ë˜ì–´ë„ ìƒë‹¨ì— ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, 
                            # ì´ê²ƒë§Œ ë³´ê³  "ë” ì´ìƒ ìµœì‹ ê¸€ ì—†ë‹¤"ê³  íŒë‹¨í•˜ë©´ ì•ˆ ë¨.
                            logger.debug(f"Skipping old sticky post: {title[:15]}...")
                        else:
                            # ì¼ë°˜ ê¸€ì¸ë° ì˜¤ë˜ë˜ì—ˆë‹¤? -> ì´í›„ ê¸€ë“¤ë„ ë‹¤ ì˜¤ë˜ë˜ì—ˆì„ í™•ë¥  ë†’ìŒ
                            found_old_normal_post = True  # ì¼ë°˜ ê¸€ ì¤‘ ì˜¤ë˜ëœ ê²ƒ ë°œê²¬ í‘œì‹œ
                            logger.debug(f"Skipping old post: {title[:15]}...")
                            pass
                else:
                    # ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨ -> ì•ˆì „í•˜ê²Œ ìˆ˜ì§‘
                    is_recent = True
                    found_recent_on_this_page = True
                    
            except Exception as e:
                is_recent = True
                found_recent_on_this_page = True
            
            # ë©”íƒ€ ë°ì´í„° íŒ¨í‚¤ì§•
            meta_data = {
                'title': title,
                'author': author,
                'timestamp': date_time
            }

            if link and is_recent:
                # ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§ ìš”ì²­
                yield response.follow(link, callback=self.parse_post, meta=meta_data)

        # í˜ì´ì§€ë„¤ì´ì…˜ (Smart Stop + Config Limit)
        
        # 1. ë‚ ì§œ ê¸°ì¤€ ì¤‘ë‹¨
        # "ìµœì‹  ê¸€"ì„ í•˜ë‚˜ë„ ëª» ì°¾ì•˜ê³ , "ì˜¤ë˜ëœ ì¼ë°˜ ê¸€"ì„ ì°¾ì•˜ë‹¤ë©´ ì¤‘ë‹¨.
        # (Stickyë§Œ ì”ëœ© ìˆì–´ì„œ ìµœì‹ ê¸€ì„ ëª» ì°¾ì€ê±°ë¼ë©´ ë‹¤ìŒ í˜ì´ì§€ë¥¼ í™•ì¸í•´ì•¼ í•¨)
        if not found_recent_on_this_page and found_old_normal_post:
            logger.info("ëª¨ë“  ê²Œì‹œë¬¼ì´ ë‚ ì§œ ê¸°ì¤€ ë¯¸ë‹¬(Sticky ì œì™¸). í˜ì´ì§€ë„¤ì´ì…˜ ì¤‘ë‹¨.")
            return

        # 2. í˜ì´ì§€ ìˆ˜ ê¸°ì¤€ ì¤‘ë‹¨ (0ì´ë©´ ë¬´ì œí•œ)
        if current_max_pages > 0 and page_count >= current_max_pages:
            logger.info(f"ê²Œì‹œíŒë³„ ìµœëŒ€ í˜ì´ì§€({current_max_pages}) ë„ë‹¬. í˜ì´ì§€ë„¤ì´ì…˜ ì¤‘ë‹¨.")
            return

        next_page = response.css("a.pageNav-jump--next::attr(href)").get()
        if next_page:
            logger.info(f"ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™ (Next Page: {page_count + 1})")
            yield response.follow(next_page, callback=self.parse, meta={'page_count': page_count + 1})

    def parse_post(self, response):
        """
        ê²Œì‹œë¬¼ ìƒì„¸ ë‚´ìš© íŒŒì‹± - XenForo Thread View.

        - LeakItemì˜ í•„ìˆ˜ í•„ë“œë¥¼ ì±„ìš°ê³  contentë¥¼ ì •ì œí•œë‹¤.
        - Hidden Content ì—¬ë¶€ë¥¼ í‘œê¸°í•´ íŒ€ì›ì´ ì‰½ê²Œ í™•ì¸í•˜ë„ë¡ í•œë‹¤.
        - ìƒì„± ë°ì´í„°ì˜ ì†Œë¹„ì²˜:
          - title/content â†’ `tricrawl/pipelines/keyword_filter.py:KeywordFilterPipeline.process_item`
          - content â†’ `tricrawl/pipelines/archive.py:ArchivePipeline._extract_contacts`
          - author/title â†’ `tricrawl/pipelines/dedup.py:DeduplicationPipeline.get_hash`
          - timestamp/category â†’ `tricrawl/pipelines/discord_notify.py:DiscordNotifyPipeline._build_embed`
        """
        item = LeakItem()
        # í•„ìˆ˜ í•„ë“œ: source/title/url/author/timestamp
        item["source"] = "DarkNetArmy"
        item["url"] = response.url
        
        # 1. ë©”íƒ€ë°ì´í„° ë³µì› (List Viewì—ì„œ ê°€ì ¸ì˜¨ ì •ë³´ ìš°ì„ )
        meta_title = response.meta.get('title')
        meta_author = response.meta.get('author')
        meta_time = response.meta.get('timestamp')
        
        # ìƒì„¸ í˜ì´ì§€ì—ì„œ ì œëª© ì¬í™•ì¸ (ë” ì •í™•í•  ìˆ˜ ìˆìŒ)
        item["title"] = (
            response.css("h1.p-title-value::text").get() or 
            meta_title or
            response.css("title::text").get()
        ).strip()
        
        # 2. ë³¸ë¬¸ ì¶”ì¶œ (ì²« ë²ˆì§¸ ê²Œì‹œë¬¼ = ì‘ì„±ê¸€)
        # XenForo: article.message--post
        # 2. ë³¸ë¬¸ ì¶”ì¶œ (ì²« ë²ˆì§¸ ê²Œì‹œë¬¼ = ì‘ì„±ê¸€)
        # XenForo: article.message--post
        posts = response.css("article.message--post") or response.css("article.message")
        
        if posts:
            first_post = posts[0]
            # ë³¸ë¬¸ ì˜ì—­: .message-content -> .bbWrapper
            content_div = first_post.css(".message-content .bbWrapper")
            
            # Hidden Content ê°ì§€ (Reaction Wall)
            hidden_block = content_div.css(".bbCodeBlock--hide")
            is_hidden = bool(hidden_block)
            
            # í…ìŠ¤íŠ¸ ì¶”ì¶œ (ì¤„ë°”ê¿ˆ ë³´ì¡´ì„ ìœ„í•´ getall í›„ ì²˜ë¦¬)
            # bbCodeBlock--hide ë‚´ë¶€ í…ìŠ¤íŠ¸("To see this hidden content...")ëŠ” ì œì™¸í•˜ê³  ì‹¶ì§€ë§Œ,
            # êµ¬ì¡°ìƒ ì„ì—¬ ìˆì„ ìˆ˜ ìˆìŒ. ì¼ë‹¨ ì „ì²´ ê°€ì ¸ì˜¤ê³  Hidden ì—¬ë¶€ í‘œì‹œ
            
            content_parts = []
            for node in content_div.css("*::text").getall():
                text = node.strip()
                if text:
                    content_parts.append(text)
            
            dirty_content = "\n".join(content_parts)
            
            # Telegram/Contact ì¶”ì¶œ (Hidden ë°–ì˜ ì •ë³´ê°€ ì¤‘ìš”)
            # a tagì˜ hrefë‚˜ í…ìŠ¤íŠ¸ì—ì„œ í…”ë ˆê·¸ë¨ ë§í¬ ì°¾ê¸°
            contacts = []
            links = content_div.css("a::attr(href)").getall()
            for link in links:
                if "t.me" in link or "telegram" in link:
                    contacts.append(link)
            
            # Hiddenì¼ ê²½ìš° ê²½ê³  ë¬¸êµ¬ ì¶”ê°€
            if is_hidden:
                dirty_content = f"ğŸ”’ [Hidden Content] (Requires Reaction)\n\n" + dirty_content
                if contacts:
                    dirty_content += f"\n\nğŸ“ Found Contacts:\n" + "\n".join(contacts)

            item["content"] = dirty_content[:5000] # ê¸¸ì´ ì œí•œ
            
            # ì‘ì„±ì (fallback)
            if not meta_author or meta_author == "Unknown":
                item["author"] = first_post.css(".message-name .username::text").get() or \
                                 first_post.css(".message-userDetails .username::text").get() or \
                                 "Unknown"
            else:
                item["author"] = meta_author
                
            # ì‹œê°„ (ëª©ë¡ í˜ì´ì§€ì—ì„œ ê°€ì ¸ì˜¨ ê°’ ìš°ì„  ì‚¬ìš© - í•„í„°ë§ ê¸°ì¤€)
            # ìƒì„¸ í˜ì´ì§€ì˜ ì‹œê°„ì€ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ (ê²Œì‹œë¬¼ ìˆ˜ì • ì‹œê°„ ë“±)
            item["timestamp"] = meta_time

            # ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ (Breadcrumbs)
            # .p-breadcrumbs -> li -> a -> span
            # ë³´í†µ ë§ˆì§€ë§‰ì—ì„œ 2ë²ˆì§¸ê°€ ê²Œì‹œíŒ ì´ë¦„ (ë§ˆì§€ë§‰ì€ í˜„ì¬ ê¸€ ì œëª©ì¼ ìˆ˜ ìˆìŒ)
            # ì—¬ê¸°ì„œëŠ” ì•ˆì „í•˜ê²Œ breadcrumbs í…ìŠ¤íŠ¸ ì „ì²´ë¥¼ ê°€ì ¸ì˜¤ê±°ë‚˜ íŠ¹ì • ìœ„ì¹˜ë¥¼ íŒŒì‹±
            breadcrumbs = response.css(".p-breadcrumbs li a span::text").getall()
            if breadcrumbs:
                # "Home > Forums > Cat > Board" í˜•íƒœ
                # ë³´í†µ ë§¨ ë’¤ê°€ ê²Œì‹œíŒ ì´ë¦„
                item["category"] = breadcrumbs[-1]
            else:
                 item["category"] = "Unknown"
                
        else:
            # êµ¬ì¡°ê°€ ë‹¤ë¥¼ ê²½ìš° ì „ì²´ í…ìŠ¤íŠ¸ fallback
            item["content"] = " ".join(response.css("body *::text").getall()).strip()[:1000]
            item["author"] = meta_author or "Unknown"
            item["timestamp"] = meta_time
            item["category"] = "Unknown"
        
        # ë°ì´í„° í´ë¦¬ë‹
        if item["author"]:
            item["author"] = item["author"].strip()
            
        yield item




