"""
LockBit 5.0 ìŠ¤íŒŒì´ë”

LockBit 5.0 (ìƒˆ ë²„ì „) ëœì„¬ì›¨ì–´ ê·¸ë£¹ Leak Site í¬ë¡¤ëŸ¬.
ê¸°ì¡´ lockbit.pyì™€ ë³„ë„ë¡œ ìš´ì˜ë¨.

ì£¼ìš” ê¸°ëŠ¥:
- ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ì—ì„œ í”¼í•´ ê¸°ì—… ì •ë³´ ì¶”ì¶œ
- ë‚ ì§œ íŒŒì‹± (ì˜ˆ: "28 Jan, 2026, 17:12 UTC")
- Views ì¶”ì¶œ
- ìƒíƒœ êµ¬ë¶„ (íƒ€ì´ë¨¸ ì§„í–‰ ì¤‘ / published)
- ì¿ í‚¤ ì§€ì› (CAPTCHA ìš°íšŒìš©)

ì°¸ê³ : LockBit 5.0ì€ CAPTCHAê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
      config/lockbit5_cookies.json íŒŒì¼ì—ì„œ ì¿ í‚¤ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
"""
import scrapy
import hashlib
import json
import re
from datetime import datetime, timezone, timedelta
from tricrawl.items import LeakItem
import yaml
from pathlib import Path
import structlog

logger = structlog.get_logger(__name__)


class LockBit5Spider(scrapy.Spider):
    """
    LockBit 5.0 ëœì„¬ì›¨ì–´ ê·¸ë£¹ Leak Site ìŠ¤íŒŒì´ë”.
    
    ì…€ë ‰í„°:
    - í”¼í•´ì ëª©ë¡: a.post-block
    - ì œëª©: .post-title
    - ì„¤ëª…: .post-block-text
    - ë‚ ì§œ: .updated-post-date span
    - ì¡°íšŒìˆ˜: .views div:last-child span
    - ìƒíƒœ: .post-timer (ì§„í–‰ ì¤‘) / .post-timer-end (published)
    """
    name = "lockbit 5.0"
    
    custom_settings = {
        "DOWNLOADER_MIDDLEWARES": {
            "tricrawl.middlewares.darknet_requests.RequestsDownloaderMiddleware": 543,
            "tricrawl.middlewares.TorProxyMiddleware": None,
            "scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware": None,
        },
        "COOKIES_ENABLED": True,
        "DOWNLOAD_DELAY": 3,
    }

    def __init__(self, *args, **kwargs):
        """YAML ì„¤ì •ì„ ë¡œë“œí•˜ê³  start_urlsë¥¼ êµ¬ì„±í•œë‹¤."""
        super().__init__(*args, **kwargs)

        self.config = {}
        self.cookies = {}
        
        try:
            project_root = Path(__file__).resolve().parents[2]
            config_path = project_root / "config" / "crawler_config.yaml"
            
            if config_path.exists():
                with open(config_path, "r", encoding="utf-8") as f:
                    full_conf = yaml.safe_load(f) or {}
                    self.config = full_conf.get('spiders', {}).get('lockbit5', {})
                logger.info(f"Config loaded from {config_path}")
            else:
                logger.warning("Config file not found, using defaults")
                
        except Exception as e:
            logger.error(f"Config load failed: {e}")

        self.target_url = self.config.get('target_url')
        if self.target_url:
            self.start_urls = [self.target_url]
        else:
            logger.error("Target URL NOT found in config for lockbit5.")
            self.start_urls = []
        
        # ì „ì—­ ì„¤ì • ì ìš©
        try:
            project_root = Path(__file__).resolve().parents[2]
            config_path = project_root / "config" / "crawler_config.yaml"
            if config_path.exists():
                with open(config_path, "r", encoding="utf-8") as f:
                    fc = yaml.safe_load(f) or {}
                    global_conf = fc.get('global', {})
                    self.days_limit = global_conf.get('days_to_crawl', 14)
            else:
                self.days_limit = 14
        except Exception:
            self.days_limit = 14
        logger.info(f"Loaded Config - Global Days: {self.days_limit}")
        
        # ì¿ í‚¤ íŒŒì¼ ë¡œë“œ (ìˆìœ¼ë©´)
        self._load_cookies()

    def _load_cookies(self):
        """config/lockbit5_cookies.jsonì—ì„œ ì¿ í‚¤ë¥¼ ë¡œë“œí•œë‹¤."""
        try:
            project_root = Path(__file__).resolve().parents[2]
            cookie_path = project_root / "config" / "lockbit5_cookies.json"
            
            if cookie_path.exists():
                with open(cookie_path, "r", encoding="utf-8") as f:
                    self.cookies = json.load(f)
                
                # ì¿ í‚¤ ìœ íš¨ì„± ê²€ì‚¬
                dcap = self.cookies.get("dcap", "")
                if not dcap or dcap == "PASTE_HERE":
                    print("\n" + "="*60)
                    print("ğŸ›‘ [ì˜¤ë¥˜] LockBit 5.0 ì¿ í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
                    print(f"âš ï¸  ì„¤ì • íŒŒì¼: {cookie_path}")
                    print("ğŸŒ ì£¼ì†Œ: http://lockbitapt67g6rwzjbcxnww5efpg4qok6vpfeth7wx3okj52ks4wtad.onion/")
                    print("ğŸ‘‰ íŒŒì¼ì„ ì—´ê³  Tor ë¸Œë¼ìš°ì €ì˜ 'dcap' ì¿ í‚¤ ê°’ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                    print("="*60 + "\n")
                    logger.error("Invalid cookie value (PASTE_HERE or empty). Stopping spider.")
                    # ìŠ¤íŒŒì´ë” ê°•ì œ ì¢…ë£Œ (CloseSpider ì˜ˆì™¸ ë°œìƒ ì‹œí‚¤ëŠ” ê²ƒì´ ì¢‹ìœ¼ë‚˜, ì—¬ê¸°ì„œ ë°”ë¡œ ë¦¬í„´í•˜ë©´ start_requestsì—ì„œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ê°€ ë¨)
                    self.cookies = {} 
                else:
                    logger.info(f"Loaded {len(self.cookies)} cookies from {cookie_path}")
            else:
                logger.warning(f"Cookie file not found at {cookie_path}")
                # í…œí”Œë¦¿ ìë™ ìƒì„±
                template = {
                    "_instructions": "Tor ë¸Œë¼ìš°ì €ì—ì„œ 'dcap' ë“±ì˜ ì¿ í‚¤ë¥¼ ë³µì‚¬í•´ ì—¬ê¸°ì— ì…ë ¥í•˜ì„¸ìš”.",
                    "dcap": "PASTE_HERE"
                }
                with open(cookie_path, "w", encoding="utf-8") as f:
                    json.dump(template, f, indent=2, ensure_ascii=False)
                logger.warning(f"âš ï¸ Created template file at {cookie_path}. Please update it with valid cookies!")
        except Exception as e:
            logger.error(f"Cookie load failed: {e}")


    def start_requests(self):
        """ì‚¬ìš©ì ë¸Œë¼ìš°ì € í—¤ë”ë¥¼ ì™„ë²½í•˜ê²Œ ëª¨ë°©í•˜ì—¬ ìš”ì²­."""
        
        # ê¸°ë³¸ í—¤ë” ì„¤ì • (ì‚¬ìš©ìê°€ ì œê³µí•œ ê°’ ê¸°ë°˜)
        headers = {
            "Host": "lockbitapt67g6rwzjbcxnww5efpg4qok6vpfeth7wx3okj52ks4wtad.onion",
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:140.0) Gecko/20100101 Firefox/140.0",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "en-US,en;q=0.5",
            #"Accept-Encoding": "gzip, deflate, br, zstd",  # requestsê°€ ìë™ ì²˜ë¦¬í•˜ë¯€ë¡œ ìƒëµ ê¶Œì¥
            #"Referer": "http://lockbitapt67g6rwzjbcxnww5efpg4qok6vpfeth7wx3okj52ks4wtad.onion/", # ì²« ìš”ì²­ì—” ìƒëµí•˜ê±°ë‚˜ ìê¸° ìì‹ 
            "Sec-GPC": "1",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
            "Sec-Fetch-Dest": "document",
            "Sec-Fetch-Mode": "navigate",
            "Sec-Fetch-Site": "same-origin",
            "Sec-Fetch-User": "?1",
            "Priority": "u=0, i"
        }

        # ì¿ í‚¤ ì¶”ê°€
        if not self.cookies:
            logger.error("No valid cookies found. Stopping spider.")
            return

        cookie_header = "; ".join([f"{k}={v}" for k, v in self.cookies.items()])
        if cookie_header:
            headers["Cookie"] = cookie_header
            logger.info(f"Cookie header set: {cookie_header[:50]}...")
        
        for url in self.start_urls:
            yield scrapy.Request(
                url=url,
                headers=headers,
                callback=self.parse,
                dont_filter=True,
            )

    def _parse_date(self, date_text: str) -> str:
        """
        ë‚ ì§œ ë¬¸ìì—´ì„ ISO 8601 í˜•ì‹ìœ¼ë¡œ ë³€í™˜.
        
        ì˜ˆì‹œ ì…ë ¥: "28 Jan, 2026, 17:12 UTC"
        ì¶œë ¥: "2026-01-28T17:12:00+00:00"
        """
        if not date_text:
            return datetime.now(timezone.utc).isoformat()
            
        try:
            # "28 Jan, 2026, 17:12 UTC" í˜•ì‹ íŒŒì‹±
            cleaned = date_text.strip().replace(" UTC", "").replace(",", "")
            # "28 Jan 2026 17:12"
            dt = datetime.strptime(cleaned, "%d %b %Y %H:%M")
            return dt.replace(tzinfo=timezone.utc).isoformat()
        except Exception as e:
            logger.debug(f"Date parse failed: {date_text} - {e}")
            return datetime.now(timezone.utc).isoformat()

    def _parse_views(self, views_text: str) -> int:
        """Views ë¬¸ìì—´ì„ ì •ìˆ˜ë¡œ ë³€í™˜."""
        if not views_text:
            return None
        try:
            # "756" -> 756, "1,234" -> 1234
            cleaned = views_text.strip().replace(",", "")
            return int(cleaned)
        except ValueError:
            return None

    def parse(self, response):
        """
        ë©”ì¸ íŒŒì„œ: a.post-block ìš”ì†Œë¥¼ ìˆœíšŒí•˜ë©° í”¼í•´ì ì •ë³´ ì¶”ì¶œ.
        """
        logger.info(f"LockBit5 Page Accessed: {response.url}")
        
        # CAPTCHA ê°ì§€
        if "captcha" in response.text.lower() or "challenge" in response.text.lower():
            logger.warning("CAPTCHA detected! Please update cookies.")
            return
        
        posts = response.css('a.post-block')
        logger.info(f"Found {len(posts)} victims.")
        
        # items/discovered í†µê³„ ê¸°ë¡
        self.crawler.stats.inc_value('items/discovered', len(posts))
        
        cutoff = datetime.now(timezone.utc) - timedelta(days=self.days_limit)
        new_items_count = 0
        
        for post in posts:
            # 1. ì œëª© (í”¼í•´ì ë„ë©”ì¸)
            title = post.css('.post-title::text').get()
            if not title:
                continue
            title = title.strip()
            
            # 2. ì„¤ëª…
            description = post.css('.post-block-text::text').get()
            description = description.strip() if description else ""
            
            # 3. ë‚ ì§œ
            date_text = post.css('.updated-post-date span::text').get()
            if date_text:
                # ì•„ì´ì½˜ ì´ë¯¸ì§€ ë‹¤ìŒ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                date_parts = post.css('.updated-post-date span::text').getall()
                date_text = " ".join([d.strip() for d in date_parts if d.strip()])
            timestamp = self._parse_date(date_text)
            
            # ë‚ ì§œ í•„í„°ë§
            try:
                dt = datetime.fromisoformat(timestamp)
                if dt < cutoff:
                    logger.debug(f"Skipping old post: {title[:30]} ({dt.date()})")
                    continue
            except Exception:
                pass
            
            # 4. ì¡°íšŒìˆ˜
            views_text = post.css('.views div:last-child span::text').get()
            views = self._parse_views(views_text)
            
            # 5. ìƒíƒœ (íƒ€ì´ë¨¸ or published)
            timer = post.css('.post-timer::text').get()
            timer_end = post.css('.post-timer-end::text').get()
            status = timer.strip() if timer else (timer_end.strip() if timer_end else "unknown")
            
            # 6. ìƒì„¸ ë§í¬
            detail_url = post.attrib.get('href', '')
            if detail_url and not detail_url.startswith('http'):
                detail_url = response.urljoin(detail_url)
            
            # dedup_id ìƒì„± (title + lockbit5 ê¸°ë°˜)
            dedup_key = f"{title}|lockbit5"
            dedup_id = hashlib.md5(dedup_key.encode()).hexdigest()
            
            # [Pre-Request Dedup] ì´ë¯¸ DBì— ìˆìœ¼ë©´ ìŠ¤í‚µ
            if hasattr(self, 'seen_ids') and dedup_id in self.seen_ids:
                logger.debug(f"Pre-skip: {title[:30]} (already in DB)")
                self.crawler.stats.inc_value('pre_dedup/skipped')
                continue
            
            new_items_count += 1
            
            # ì½˜í…ì¸ ì— ìƒíƒœ ì •ë³´ í¬í•¨
            content = f"{description}\n\n[Status: {status}]"
            
            yield LeakItem(
                source="LockBit5",
                title=title,
                url=detail_url or response.url,
                author="LockBit Group",
                timestamp=timestamp,
                content=content,
                category="Ransomware",
                site_type="Ransomware",
                dedup_id=dedup_id,
                views=views,
            )
        
        logger.info(f"[LockBit5] Page complete: {new_items_count} new items")
