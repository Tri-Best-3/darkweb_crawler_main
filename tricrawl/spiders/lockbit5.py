"""
LockBit 5.0 Spider
Target: LockBit 5.0 Leak Site (New Version)
Features: Cookie support (CAPTCHA bypass), Date parsing, Status tracking
"""
import scrapy
import hashlib
import json
import re
from datetime import datetime, timezone, timedelta
from scrapy.exceptions import CloseSpider
from tricrawl.items import LeakItem
import yaml
from pathlib import Path
import structlog

logger = structlog.get_logger(__name__)


class LockBit5Spider(scrapy.Spider):
    """
    LockBit 5.0 Ransomware Spider
    """
    name = "lockbit 5.0"
    
    custom_settings = {
        "DOWNLOADER_MIDDLEWARES": {
            "tricrawl.middlewares.darknet_requests.RequestsDownloaderMiddleware": 543,
            "tricrawl.middlewares.TorProxyMiddleware": None,
            "scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware": None,
        },
        "COOKIES_ENABLED": True,
        "DOWNLOAD_DELAY": 3,
        "DOWNLOAD_TIMEOUT": 30,  # ì¿ í‚¤ ë§Œë£Œ ì‹œ ë¹ ë¥¸ ì‹¤íŒ¨ (ê¸°ë³¸ 180ì´ˆ â†’ 30ì´ˆ)
    }

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.setup_alerts = []  # UIì— í‘œì‹œí•  ê²½ê³  ë¦¬ìŠ¤íŠ¸

        self.config = {}
        self.cookies = {}
        
        try:
            project_root = Path(__file__).resolve().parents[2]
            config_path = project_root / "config" / "crawler_config.yaml"
            
            if config_path.exists():
                with open(config_path, "r", encoding="utf-8") as f:
                    full_conf = yaml.safe_load(f) or {}
                    self.config = full_conf.get('spiders', {}).get('lockbit5', {})
                logger.info(f"Config loaded from {config_path}")
            else:
                logger.warning("Config file not found, using defaults")
                
        except Exception as e:
            logger.error(f"Config load failed: {e}")

        self.target_url = self.config.get('target_url')
        if self.target_url:
            self.start_urls = [self.target_url]
        else:
            logger.error("Target URL NOT found in config for lockbit5.")
            self.start_urls = []
        
        # Global configs
        try:
            project_root = Path(__file__).resolve().parents[2]
            config_path = project_root / "config" / "crawler_config.yaml"
            if config_path.exists():
                with open(config_path, "r", encoding="utf-8") as f:
                    fc = yaml.safe_load(f) or {}
                    global_conf = fc.get('global', {})
                    self.days_limit = global_conf.get('days_to_crawl', 14)
            else:
                self.days_limit = 14
        except Exception:
            self.days_limit = 14
        logger.info(f"Loaded Config - Global Days: {self.days_limit}")
        
        # Load cookies if available
        self._load_cookies()

    def _load_cookies(self):
        """Load cookies from config/lockbit5_cookies.json"""
        try:
            project_root = Path(__file__).resolve().parents[2]
            cookie_path = project_root / "config" / "lockbit5_cookies.json"
            
            if cookie_path.exists():
                with open(cookie_path, "r", encoding="utf-8") as f:
                    self.cookies = json.load(f)
                
                # ì¿ í‚¤ ìœ íš¨ì„± ê²€ì‚¬
                dcap = self.cookies.get("dcap", "")
                if not dcap or dcap == "PASTE_HERE":
                    msg = f"[bold red]âœ— LockBit 5.0 ì¿ í‚¤ ë¯¸ì„¤ì •[/bold red] ({cookie_path.name})"
                    self.setup_alerts.append(msg)
                    logger.error(
                        "LockBit 5.0 Cookie Missing",
                        config_path=str(cookie_path),
                        instruction="Please update 'dcap' cookie in the JSON file."
                    )
                    self.cookies = {} 
                else:
                    logger.info(f"Loaded {len(self.cookies)} cookies from {cookie_path}")
            else:
                logger.warning(f"Cookie file not found at {cookie_path}")
                # í…œí”Œë¦¿ ìë™ ìƒì„±
                template = {
                    "_instructions": "Tor ë¸Œë¼ìš°ì €ì—ì„œ 'dcap' ë“±ì˜ ì¿ í‚¤ë¥¼ ë³µì‚¬í•´ ì—¬ê¸°ì— ì…ë ¥í•˜ì„¸ìš”.",
                    "dcap": "PASTE_HERE"
                }
                with open(cookie_path, "w", encoding="utf-8") as f:
                    json.dump(template, f, indent=2, ensure_ascii=False)
                logger.warning(f"âš ï¸ Created template file at {cookie_path}. Please update it with valid cookies!")
        except Exception as e:
            logger.error(f"Cookie load failed: {e}")


    def start_requests(self):
        """Emulate browser headers and existing cookies"""
        
        # ê¸°ë³¸ í—¤ë” ì„¤ì • (ì‚¬ìš©ìê°€ ì œê³µí•œ ê°’ ê¸°ë°˜)
        headers = {
            "Host": "lockbitapt67g6rwzjbcxnww5efpg4qok6vpfeth7wx3okj52ks4wtad.onion",
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:140.0) Gecko/20100101 Firefox/140.0",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "en-US,en;q=0.5",
            #"Accept-Encoding": "gzip, deflate, br, zstd",  # requestsê°€ ìë™ ì²˜ë¦¬í•˜ë¯€ë¡œ ìƒëµ ê¶Œì¥
            #"Referer": "http://lockbitapt67g6rwzjbcxnww5efpg4qok6vpfeth7wx3okj52ks4wtad.onion/", # ì²« ìš”ì²­ì—” ìƒëµí•˜ê±°ë‚˜ ìê¸° ìì‹ 
            "Sec-GPC": "1",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
            "Sec-Fetch-Dest": "document",
            "Sec-Fetch-Mode": "navigate",
            "Sec-Fetch-Site": "same-origin",
            "Sec-Fetch-User": "?1",
            "Priority": "u=0, i"
        }

        # ì¿ í‚¤ ì¶”ê°€
        if not self.cookies:
            logger.error("No valid cookies found. Stopping spider.")
            return

        cookie_header = "; ".join([f"{k}={v}" for k, v in self.cookies.items()])
        if cookie_header:
            headers["Cookie"] = cookie_header
            logger.info(f"Cookie header set: {cookie_header[:50]}...")
        
        for url in self.start_urls:
            yield scrapy.Request(
                url=url,
                headers=headers,
                callback=self.parse,
                dont_filter=True,
            )

    def _parse_date(self, date_text: str) -> str:
        """Parse date string to ISO 8601"""
        if not date_text:
            return datetime.now(timezone.utc).isoformat()
            
        try:
            # "28 Jan, 2026, 17:12 UTC" í˜•ì‹ íŒŒì‹±
            cleaned = date_text.strip().replace(" UTC", "").replace(",", "")
            # "28 Jan 2026 17:12"
            dt = datetime.strptime(cleaned, "%d %b %Y %H:%M")
            return dt.replace(tzinfo=timezone.utc).isoformat()
        except Exception as e:
            logger.debug(f"Date parse failed: {date_text} - {e}")
            return datetime.now(timezone.utc).isoformat()

    def _parse_views(self, views_text: str) -> int:
        """Views ë¬¸ìì—´ì„ ì •ìˆ˜ë¡œ ë³€í™˜."""
        if not views_text:
            return None
        try:
            # "756" -> 756, "1,234" -> 1234
            cleaned = views_text.strip().replace(",", "")
            return int(cleaned)
        except ValueError:
            return None

    def parse(self, response):
        """Main parser for victim list"""
        logger.info(f"LockBit5 Page Accessed: {response.url}")
        
        # CAPTCHA/ì¸ì¦ ì‹¤íŒ¨ ê°ì§€ â†’ ì¦‰ì‹œ ì¢…ë£Œ
        body_lower = response.text.lower()
        if "captcha" in body_lower or "challenge" in body_lower or len(response.text) < 500:
            logger.error("ğŸ›‘ Cookie expired or CAPTCHA detected! Please update cookies.")
            print("\n" + "="*60)
            print("ğŸ›‘ [ì˜¤ë¥˜] LockBit 5.0 ì¿ í‚¤ê°€ ë§Œë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            print("ğŸ‘‰ Tor ë¸Œë¼ìš°ì €ì—ì„œ ìƒˆ ì¿ í‚¤ë¥¼ ë³µì‚¬í•´ì£¼ì„¸ìš”.")
            print("="*60 + "\n")
            raise CloseSpider("cookie_expired")
        
        posts = response.css('a.post-block')
        logger.info(f"Found {len(posts)} victims.")
        
        # items/discovered í†µê³„ ê¸°ë¡
        self.crawler.stats.inc_value('items/discovered', len(posts))
        
        cutoff = datetime.now(timezone.utc) - timedelta(days=self.days_limit)
        new_items_count = 0
        
        for post in posts:
            # 1. ì œëª© (í”¼í•´ì ë„ë©”ì¸)
            title = post.css('.post-title::text').get()
            if not title:
                continue
            title = title.strip()
            
            # 2. ì„¤ëª…
            description = post.css('.post-block-text::text').get()
            description = description.strip() if description else ""
            
            # 3. ë‚ ì§œ
            date_text = post.css('.updated-post-date span::text').get()
            if date_text:
                # ì•„ì´ì½˜ ì´ë¯¸ì§€ ë‹¤ìŒ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                date_parts = post.css('.updated-post-date span::text').getall()
                date_text = " ".join([d.strip() for d in date_parts if d.strip()])
            timestamp = self._parse_date(date_text)
            
            # ë‚ ì§œ í•„í„°ë§
            try:
                dt = datetime.fromisoformat(timestamp)
                if dt < cutoff:
                    logger.debug(f"Skipping old post: {title[:30]} ({dt.date()})")
                    continue
            except Exception:
                pass
            
            # 4. ì¡°íšŒìˆ˜
            views_text = post.css('.views div:last-child span::text').get()
            views = self._parse_views(views_text)
            
            # 5. ìƒíƒœ (íƒ€ì´ë¨¸ or published)
            timer = post.css('.post-timer::text').get()
            timer_end = post.css('.post-timer-end::text').get()
            status = timer.strip() if timer else (timer_end.strip() if timer_end else "unknown")
            
            # 6. ìƒì„¸ ë§í¬
            detail_url = post.attrib.get('href', '')
            if detail_url and not detail_url.startswith('http'):
                detail_url = response.urljoin(detail_url)
            
            # dedup_id ìƒì„± (title + lockbit5 ê¸°ë°˜)
            dedup_key = f"{title}|lockbit5"
            dedup_id = hashlib.md5(dedup_key.encode()).hexdigest()
            
            # [Pre-Request Dedup] ì´ë¯¸ DBì— ìˆìœ¼ë©´ ìŠ¤í‚µ
            if hasattr(self, 'seen_ids') and dedup_id in self.seen_ids:
                logger.debug(f"Pre-skip: {title[:30]} (already in DB)")
                self.crawler.stats.inc_value('pre_dedup/skipped')
                continue
            
            new_items_count += 1
            
            # ì½˜í…ì¸ ì— ìƒíƒœ ì •ë³´ í¬í•¨
            content = f"{description}\n\n[Status: {status}]"
            
            yield LeakItem(
                source="LockBit5",
                title=title,
                url=detail_url or response.url,
                author="LockBit Group",
                timestamp=timestamp,
                content=content,
                category="Ransomware",
                site_type="Ransomware",
                dedup_id=dedup_id,
                views=views,
            )
        
        logger.info(f"[LockBit5] Page complete: {new_items_count} new items")
